{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import base64\n",
    "from openai import AzureOpenAI  \n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "\n",
    "\n",
    "endpoint = os.getenv(\"\", \"\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")  \n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication    \n",
    "client = AzureOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version = \"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "    \n",
    "# Generate the completion  \n",
    "def chat_completion_4o(USER_MSG):\n",
    "    completion = client.chat.completions.create(  \n",
    "        model=deployment,\n",
    "        messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are going to identify the corresponding sense tag of an ambiguous word.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": USER_MSG\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "] ,\n",
    "        max_tokens=500,  \n",
    "        temperature=0.0,\n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,\n",
    "        stop=None,  \n",
    "        stream=False\n",
    "        )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5569b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = \"\"\n",
    "model_name = \"gpt-4o\"\n",
    "deployment = \"gpt-4o\"\n",
    "\n",
    "subscription_key = \"\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "# Generate the completion  \n",
    "def chat_completion_4o(USER_MSG):\n",
    "    completion = client.chat.completions.create(  \n",
    "        model=deployment,\n",
    "        messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are going to identify the corresponding sense tag of an ambiguous word.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": USER_MSG\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "] ,\n",
    "        max_tokens=500,  \n",
    "        temperature=0.0,\n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,\n",
    "        stop=None,  \n",
    "        stream=False\n",
    "        )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together() # auth defaults to os.environ.get(\"TOGETHER_API_KEY\")\n",
    "\n",
    "def chat_completion_deepseek(user_msg: str) -> str:   \n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"deepseek-ai/DeepSeek-V3\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": user_msg\n",
    "        }\n",
    "      ]\n",
    "  )\n",
    "  return(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain langchain-core\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec87bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and labels\n",
    "file_paths = {\n",
    "    \"ambiguous\": \"human_ambigous.txt\",\n",
    "    \"easy\": \"human_easy.txt\",\n",
    "    \"unlikely\": \"human_unlikely.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5def1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(file_path):\n",
    "    \"\"\"Load and split a text file using \\n as separator.\"\"\"\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\"],\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2022e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(store_name):\n",
    "    \"\"\"Load an existing FAISS vector store from disk.\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "        model_kwargs={'device': 'cpu'},  # change to 'cuda' if you have GPU\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    return FAISS.load_local(\n",
    "        f\"vectorstore_{store_name}\",\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # required for FAISS in some LangChain versions\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load all your stores\n",
    "# -------------------------------\n",
    "vector_stores = {\n",
    "    \"ambiguous\": load_vector_store(\"ambiguous\"),\n",
    "    \"easy\": load_vector_store(\"easy\"),\n",
    "    \"unlikely\": load_vector_store(\"unlikely\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_all_stores(query, k=1):\n",
    "    \"\"\"Retrieve top-k results from each vector store.\"\"\"\n",
    "    results = {}\n",
    "    for label, store in vector_stores.items():\n",
    "        docs = store.similarity_search(query, k=k)\n",
    "        results[label] = docs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents from each vector store\n",
    "    and return them as combined and structured strings.\n",
    "    \"\"\"\n",
    "    retrieved = retrieve_from_all_stores(query, k=k)\n",
    "\n",
    "    retrieved_texts = {}\n",
    "    for label, docs in retrieved.items():\n",
    "        if docs:\n",
    "            # Join all retrieved chunks for that store\n",
    "            combined_chunks = \"\\n---\\n\".join([d.page_content.strip() for d in docs])\n",
    "            retrieved_texts[label] = combined_chunks\n",
    "        else:\n",
    "            retrieved_texts[label] = \"\"\n",
    "\n",
    "    # Combine everything into one formatted string\n",
    "    combined_text = \"\\n\\n\".join(\n",
    "        [f\"### {label.upper()} CONTEXT ###\\n{content}\"\n",
    "         for label, content in retrieved_texts.items() if content]\n",
    "    )\n",
    "\n",
    "    return combined_text, retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a pair of parallel rails providing a runway for wheels,The detectives arrived at the abandoned train station. They were looking for signs of the missing artifact. A faint trail caught their attention.,They followed the <WSD>track</WSD>.,They began to run along the abandoned railway line, hopping from wooden sleeper to sleeper to avoid twisting an ankle.How do humans react to ambiguous situations?\"\n",
    "combined_text, retrieved_texts = rag_pipeline(query)\n",
    "\n",
    "print(\"üîπ Combined Text:\\n\")\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a079019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = ''\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "            \n",
    "            pre_context = f''' **Story:**  Precontext: {precontext} - Sentence: {sentence} - Ending: {ending}  Homonym Word: \"{homonym}'''\n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''Your task is to simulate the judgments of 5 independent human annotators who are rating the plausibility of a proposed meaning of a homonymous word on a 1 to 5 scale, based on a short story.\n",
    "\n",
    "Each annotator reasons independently but follows the same structured reasoning process described below. Differences among annotator scores should naturally arise from ambiguity, weak evidence, or conflicting cues in the story.\n",
    "\n",
    "You will be provided with example stories illustrating the scoring rubric at different plausibility levels. In those examples, ‚ÄúAnnotations by 5 annotators‚Äù demonstrate how agreement or disagreement emerges depending on contextual clarity.\n",
    "{combined_text}\n",
    "\n",
    "Now evaluate the following story against the proposed meaning.\n",
    "\n",
    "**Story:**\n",
    "* **Precontext:** {precontext}\n",
    "* **Sentence:** {sentence}\n",
    "* **Ending:** {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "### Thinking Process & Response Format\n",
    "\n",
    "*Complete each step of this process in your analysis. Each annotator completes the following steps internally before assigning a score.*\n",
    "\n",
    "**Step 1: Initial Context Analysis**\n",
    "* Read the **Precontext** *alone*. Which meaning(s) of \"{homonym}\" does it make you expect?\n",
    "* Read the **Ambiguous Sentence** *alone*. Is the homonym part of a common idiom (e.g., \"had guts\") or a set phrase (e.g., \"drawing water\")?\n",
    "\n",
    "**Step 2: The Ending as a Clarifier**\n",
    "* Now, read the **Ending**. How does this new information *shift* the story?\n",
    "* Does the Ending strongly *confirm* one meaning, *contradict* another, or introduce a *twist*?\n",
    "* **Crucially:** Does the Ending act as a *paraphrase* or *definition* of a figurative or idiomatic meaning (e.g., an ending of \"She was very brave\" is a direct definition for the meaning \"fortitude\")?\n",
    "\n",
    "**Step 3: Holistic Coherence Check**\n",
    "* Read all three parts (Precontext, Sentence, Ending) as a *single, coherent narrative*.\n",
    "* Can all parts co-exist with the **Proposed Meaning**? Or do some parts create a conflict?\n",
    "* **Supporting Clues:** List the specific words/phrases from the *entire* story (Precontext, Sentence, or Ending) that support the **Proposed Meaning**.\n",
    "* **Conflicting Clues:** List the specific words/phrases that contradict or make the **Proposed Meaning** awkward or implausible.\n",
    "\n",
    "**Step 4: Final Score and Justification**\n",
    "* Based on your coherence check in Step 3, assign a score.\n",
    "* *Pay close attention to contradictions*: If a strong clue in the Ending directly conflicts with a meaning , the score must be low, *even if* the precontext supported it.\n",
    "\n",
    "\n",
    "**Scoring Rubric:**\n",
    "* **5: Perfectly plausible.** The meaning is strongly supported by the *entire* context, and all parts of the story (Precontext, Sentence, Ending) form a consistent, logical narrative.\n",
    "* **4: Very plausible.** The meaning fits well and is consistent. There might be minor ambiguity, but no real contradictions.\n",
    "* **3: Moderately plausible.** The meaning is possible, but the context is ambiguous or contains *minor* conflicting clues that make the story a bit strange.\n",
    "* **2: Barely plausible.** The meaning *largely* conflicts with the context. You have to ignore a key piece of information (especially in the Ending) to make it fit.\n",
    "* **1: Implausible.** The meaning is directly and strongly contradicted by the context (e.g., the Ending makes it physically impossible or nonsensical).\n",
    "\n",
    "Perform all reasoning privately. Simulate the final judgments of 5 annotators, reflecting:\n",
    "\n",
    "High agreement when the meaning is clear,\n",
    "Partial disagreement when the story is ambiguous,\n",
    "Strong disagreement when cues conflict.\n",
    "\n",
    "Output ONLY five comma-separated integers between 1 and 5 (e.g., 5,5,4,5,4). Do not include explanations, text, or formatting beyond the scores.'''\n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            #print(prompt)\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            full_text_fin=full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {full_text_fin}\")\n",
    "            line_num += 1\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2598030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = ''\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "            \n",
    "            pre_context = f''' **Story:**  Precontext: {precontext} - Sentence: {sentence} - Ending: {ending}  Homonym Word: \"{homonym}'''\n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''Your task is to simulate the judgments of 5 independent human annotators who are rating the plausibility of a proposed meaning of a homonymous word on a 1 to 5 scale, based on a short story.\n",
    "\n",
    "Each annotator reasons independently but follows the same structured reasoning process described below. Differences among annotator scores should naturally arise from ambiguity, weak evidence, or conflicting cues in the story.\n",
    "\n",
    "You will be provided with example stories illustrating the scoring rubric at different plausibility levels. In those examples, ‚ÄúAnnotations by 5 annotators‚Äù demonstrate how agreement or disagreement emerges depending on contextual clarity. Also at the end, final Plausibility Score is provided which is the most probable output based on the average score of 5 annotators.\n",
    "{combined_text}\n",
    "\n",
    "Now simulate the evaluation the following story against the proposed meaning. Follow the given instructions.\n",
    "\n",
    "**Story:**\n",
    "* **Precontext:** {precontext}\n",
    "* **Sentence:** {sentence}\n",
    "* **Ending:** {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "\n",
    "Complete each step of this process in your analysis.\n",
    "\n",
    "1. Read the **Precontext** *alone*. Which meaning(s) of \"{homonym}\" does it make you expect?\n",
    "2. Read the **Ambiguous Sentence** *alone*. Decide whether the homonym part is a common idiom or a set phrase.\n",
    "3. Now, read the **Ending**. How does this new information *shift* the story?\n",
    "4. Does the Ending strongly *confirm* one meaning, *contradict* another, or introduce a *twist*?\n",
    "5. Now Read all three parts (Precontext, Sentence, Ending) as a *single, coherent narrative*.\n",
    "6. Decide whether all parts co-exist with the **Proposed Meaning**? Or do some parts create a conflict?\n",
    "7. Based on the above steps, Assign a final score.\n",
    "\n",
    "\n",
    "**Scoring Rubric:**\n",
    "1: The displayed meaning is not plausible at all given the context.\n",
    "2: The displayed meaning is theoretically conceivable, but less plausible than other meanings.\n",
    "3: The displayed meaning represents one of multiple, similarly plausible interpretations.\n",
    "4: The displayed meaning represents the most plausible interpretation; other meanings may still be conceivable.\n",
    "5: The displayed meaning is the only plausible meaning given the context.\n",
    "\n",
    "Perform all reasoning privately. Simulate the final judgments of 5 annotators and decide the final plausibility score.\n",
    "\n",
    "Output ONLY single integer between 1 and 5 (e.g., 3). Do not include explanations, text, or formatting beyond the scores.'''\n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            #print(prompt)\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            full_text_fin=full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {full_text_fin}\")\n",
    "            line_num += 1\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c410a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = ''\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "            \n",
    "            pre_context = f''' **Story:**  Precontext: {precontext} - Sentence: {sentence} - Ending: {ending}  Homonym Word: \"{homonym}'''\n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''Your task is to simulate the judgments of 5 independent human annotators who are rating the plausibility of a proposed meaning of a homonymous word on a 1 to 5 scale, based on a short story.\n",
    "\n",
    "Each annotator reasons independently but follows the same structured reasoning process described below. Differences among annotator scores should naturally arise from ambiguity, weak evidence, or conflicting cues in the story.\n",
    "\n",
    "You will be provided with example stories illustrating the scoring rubric at different plausibility levels. In those examples, ‚ÄúAnnotations by 5 annotators‚Äù demonstrate how agreement or disagreement emerges depending on contextual clarity.\n",
    "{combined_text}\n",
    "\n",
    "Now evaluate the following story against the proposed meaning.\n",
    "\n",
    "**Story:**\n",
    "* **Precontext:** {precontext}\n",
    "* **Sentence:** {sentence}\n",
    "* **Ending:** {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "### Thinking Process & Response Format\n",
    "\n",
    "*Complete each step of this process in your analysis. Each annotator completes the following steps internally before assigning a score.*\n",
    "\n",
    "**Step 1: Initial Context Analysis**\n",
    "* Read the **Precontext** *alone*. Which meaning(s) of \"{homonym}\" does it make you expect?\n",
    "* Read the **Ambiguous Sentence** *alone*. Is the homonym part of a common idiom (e.g., \"had guts\") or a set phrase (e.g., \"drawing water\")?\n",
    "\n",
    "**Step 2: The Ending as a Clarifier**\n",
    "* Now, read the **Ending**. How does this new information *shift* the story?\n",
    "* Does the Ending strongly *confirm* one meaning, *contradict* another, or introduce a *twist*?\n",
    "* **Crucially:** Does the Ending act as a *paraphrase* or *definition* of a figurative or idiomatic meaning (e.g., an ending of \"She was very brave\" is a direct definition for the meaning \"fortitude\")?\n",
    "\n",
    "**Step 3: Holistic Coherence Check**\n",
    "* Read all three parts (Precontext, Sentence, Ending) as a *single, coherent narrative*.\n",
    "* Can all parts co-exist with the **Proposed Meaning**? Or do some parts create a conflict?\n",
    "* **Supporting Clues:** List the specific words/phrases from the *entire* story (Precontext, Sentence, or Ending) that support the **Proposed Meaning**.\n",
    "* **Conflicting Clues:** List the specific words/phrases that contradict or make the **Proposed Meaning** awkward or implausible.\n",
    "\n",
    "**Step 4: Final Score and Justification**\n",
    "* Based on your coherence check in Step 3, assign a score.\n",
    "* *Pay close attention to contradictions*: If a strong clue in the Ending directly conflicts with a meaning , the score must be low, *even if* the precontext supported it.\n",
    "\n",
    "\n",
    "**Scoring Rubric:**\n",
    "* **5: Perfectly plausible.** The meaning is strongly supported by the *entire* context, and all parts of the story (Precontext, Sentence, Ending) form a consistent, logical narrative.\n",
    "* **4: Very plausible.** The meaning fits well and is consistent. There might be minor ambiguity, but no real contradictions.\n",
    "* **3: Moderately plausible.** The meaning is possible, but the context is ambiguous or contains *minor* conflicting clues that make the story a bit strange.\n",
    "* **2: Barely plausible.** The meaning *largely* conflicts with the context. You have to ignore a key piece of information (especially in the Ending) to make it fit.\n",
    "* **1: Implausible.** The meaning is directly and strongly contradicted by the context (e.g., the Ending makes it physically impossible or nonsensical).\n",
    "\n",
    "Perform all reasoning privately. Simulate the final judgments of 5 annotators, reflecting:\n",
    "\n",
    "High agreement when the meaning is clear,\n",
    "Partial disagreement when the story is ambiguous,\n",
    "Strong disagreement when cues conflict or story is implausible.\n",
    "\n",
    "Output ONLY five comma-separated integers between 1 and 5 (e.g., 3,5,4,5,4). Do not include explanations, text, or formatting beyond the scores.'''\n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            #print(prompt)\n",
    "            full_text = chat_completion_deepseek(prompt)\n",
    "            full_text_fin=full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {full_text_fin}\")\n",
    "            line_num += 1\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_predictions_to_jsonl(numbers, file_path):\n",
    "    \"\"\"\n",
    "    Saves a list of numbers to a JSONL file in the format:\n",
    "    {\"id\": \"<index>\", \"prediction\": <number>}\n",
    "    \n",
    "    Args:\n",
    "        numbers (list): List of numbers (predictions).\n",
    "        file_path (str): Path to output .jsonl file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for idx, num in enumerate(numbers):\n",
    "            record = {\n",
    "                \"id\": str(idx),\n",
    "                \"prediction\": num\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "from collections import Counter\n",
    "with open('results_output_GPT.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for item in lines:\n",
    "        elements = item.split(': ')[1].strip()\n",
    "        list_numbers = [int(x.strip()) for x in elements.split(',')]\n",
    "        # predicts.append(sum(list_numbers)/5)\n",
    "        most_frequent = Counter(list_numbers).most_common(1)[0][0]\n",
    "        predicts.append(most_frequent)\n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "save_predictions_to_jsonl(predicts, 'predictions_GPT_most.jsonl')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictsdeep=[]\n",
    "from collections import Counter\n",
    "with open('resultoutput_deepseek.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for item in lines:\n",
    "        elements = item.split(': ')[1].strip()\n",
    "        list_numbers = [int(x.strip()) for x in elements.split(',')]\n",
    "        # predictsdeep.append(sum(list_numbers)/5)\n",
    "        most_frequent = Counter(list_numbers).most_common(1)[0][0]\n",
    "        predictsdeep.append(most_frequent)\n",
    "print(predictsdeep)\n",
    "print(len(predictsdeep))\n",
    "# save_predictions_to_jsonl(predictsdeep, 'predictions_deepseek.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictsfinal=[]\n",
    "for gpt, deep in zip(predicts, predictsdeep):\n",
    "    mean = (gpt + deep) / 2\n",
    "    predictsfinal.append(round(mean))\n",
    "print(predictsfinal)\n",
    "save_predictions_to_jsonl(predictsfinal, 'predictions_final.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ec8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt=[5, 2, 5, 3, 5, 3, 2, 3, 2, 3, 2, 3, 3, 3, 1, 5, 3, 4, 5, 4, 5, 5, 5, 4, 4, 2, 3, 2, 4, 2, 3, 5, 3, 5, 4, 5, 4, 4, 3, 5, 4, 5, 3, 4, 3, 4, 3, 4, 4, 2, 3, 5, 3, 5, 4, 4, 3, 5, 4, 5, 5, 2, 2, 5, 4, 2, 4, 2, 1, 5, 2, 2, 5, 2, 2, 5, 5, 4, 5, 3, 2, 5, 4, 5, 2, 5, 2, 5, 2, 5, 2, 3, 2, 4, 3, 4, 4, 2, 5, 2, 5, 3, 5, 2, 2, 5, 2, 5, 5, 1, 4, 3, 5, 2, 5, 1, 2, 5, 5, 1, 5, 3, 5, 3, 5, 4, 5, 3, 2, 4, 2, 4, 5, 2, 2, 3, 3, 3, 5, 5, 4, 5, 4, 5, 2, 5, 2, 5, 2, 5, 5, 2, 2, 5, 5, 2, 5, 2, 2, 5, 5, 2, 2, 5, 2, 5, 2, 5, 3, 1, 5, 1, 3, 1, 5, 2, 1, 4, 2, 3, 4, 3, 2, 4, 3, 4, 3, 2, 2, 5, 3, 3, 5, 2, 2, 3, 4, 2, 4, 2, 3, 2, 4, 2, 4, 2, 2, 5, 3, 3, 3, 2, 3, 3, 3, 3, 5, 3, 2, 4, 3, 4, 5, 2, 3, 4, 4, 4, 2, 3, 2, 3, 2, 3, 5, 5, 3, 5, 4, 5, 2, 5, 2, 5, 2, 5, 5, 2, 2, 2, 5, 2, 5, 3, 5, 2, 5, 4, 2, 5, 2, 5, 2, 5, 3, 3, 2, 3, 3, 5, 5, 2, 2, 5, 3, 3, 2, 5, 2, 5, 2, 4, 3, 5, 3, 5, 4, 5, 5, 4, 2, 5, 4, 5, 2, 2, 2, 5, 2, 5, 3, 2, 3, 5, 4, 3, 4, 2, 3, 3, 4, 3, 3, 3, 2, 3, 2, 3, 5, 2, 5, 2, 5, 2, 5, 2, 4, 2, 5, 2, 4, 2, 1, 5, 2, 4, 5, 2, 2, 5, 4, 2, 5, 2, 3, 4, 4, 3, 5, 2, 2, 3, 2, 2, 5, 2, 4, 3, 4, 2, 5, 2, 5, 1, 3, 2, 4, 2, 1, 4, 2, 3, 5, 2, 5, 2, 3, 3, 3, 2, 2, 3, 3, 2, 5, 1, 4, 2, 5, 1, 5, 2, 2, 4, 5, 2, 5, 2, 4, 4, 5, 3, 2, 5, 2, 4, 2, 4, 2, 1, 2, 3, 2, 1, 5, 1, 3, 2, 5, 2, 2, 3, 4, 3, 3, 3, 5, 2, 4, 3, 4, 3, 4, 2, 4, 2, 3, 2, 5, 2, 4, 3, 4, 3, 4, 2, 3, 2, 4, 3, 4, 2, 2, 5, 4, 4, 4, 2, 3, 5, 4, 3, 2, 5, 3, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 3, 5, 3, 4, 3, 5, 2, 2, 5, 2, 3, 2, 4, 1, 5, 2, 4, 5, 3, 5, 2, 5, 2, 4, 2, 2, 5, 2, 4, 5, 2, 2, 4, 2, 2, 5, 3, 2, 5, 2, 4, 5, 2, 2, 5, 2, 4, 2, 3, 1, 5, 2, 4, 2, 3, 1, 5, 3, 4, 5, 5, 5, 5, 4, 5, 2, 4, 2, 5, 2, 5, 2, 2, 2, 5, 2, 5, 1, 4, 2, 5, 2, 5, 5, 2, 1, 5, 2, 4, 5, 5, 4, 4, 4, 5, 5, 2, 3, 4, 4, 3, 5, 2, 2, 5, 2, 5, 4, 1, 2, 2, 3, 1, 2, 4, 2, 5, 2, 4, 2, 4, 2, 4, 2, 4, 5, 1, 5, 2, 5, 1, 3, 2, 1, 5, 2, 4, 5, 2, 2, 5, 2, 4, 5, 2, 3, 4, 3, 4, 3, 4, 2, 5, 3, 4, 5, 2, 5, 2, 4, 2, 3, 2, 2, 2, 4, 2, 5, 2, 3, 3, 5, 2, 2, 4, 2, 5, 3, 4, 4, 2, 2, 5, 3, 4, 4, 3, 2, 4, 3, 4, 5, 2, 3, 5, 5, 2, 4, 5, 2, 5, 3, 5, 5, 5, 2, 5, 3, 5, 2, 5, 2, 5, 2, 5, 3, 3, 2, 5, 2, 4, 3, 5, 3, 5, 2, 5, 2, 2, 2, 4, 2, 4, 5, 1, 2, 4, 3, 1, 5, 5, 2, 4, 3, 4, 3, 4, 4, 5, 3, 5, 4, 2, 2, 5, 4, 2, 5, 2, 2, 5, 3, 4, 5, 1, 2, 5, 5, 3, 5, 1, 5, 2, 5, 2, 2, 2, 1, 5, 2, 5, 5, 2, 2, 5, 3, 3, 5, 1, 5, 2, 5, 2, 2, 2, 2, 5, 4, 3, 3, 5, 5, 2, 4, 3, 5, 2, 2, 5, 3, 3, 2, 2, 2, 5, 2, 5, 4, 2, 2, 5, 2, 4, 2, 5, 4, 4, 4, 3, 3, 3, 2, 3, 2, 4, 5, 2, 2, 4, 1, 4, 5, 3, 3, 4, 3, 4, 1, 5, 2, 5, 2, 5, 5, 3, 3, 4, 3, 4, 2, 2, 2, 5, 2, 4, 4, 2, 2, 3, 2, 3, 5, 2, 2, 5, 3, 3, 4, 2, 4, 4, 4, 3, 4, 2, 3, 2, 3, 2, 2, 5, 2, 4, 2, 5, 5, 1, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 4, 2, 3, 2, 5, 2, 2, 4, 4, 4, 5, 2, 2, 3, 3, 2, 2, 4, 2, 4, 2, 4, 5, 2, 2, 5, 4, 2, 3, 2, 1, 5, 2, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 4, 3]\n",
    "\n",
    "count = 0\n",
    "for gpt, predict in zip(gpt, predictsdeep):\n",
    "    if gpt == predict:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictsdeep=[]\n",
    "from collections import Counter\n",
    "with open('GPT_with_new.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for item in lines:\n",
    "        elements = item.split(': ')[1].strip()\n",
    "        predictsdeep.append(int(elements))\n",
    "print(predictsdeep)\n",
    "print(len(predictsdeep))\n",
    "save_predictions_to_jsonl(predictsdeep, 'predictions_gpt_new_rationale.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
