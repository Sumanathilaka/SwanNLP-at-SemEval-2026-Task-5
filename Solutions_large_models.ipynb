{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import base64\n",
    "from openai import AzureOpenAI  \n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "\n",
    "endpoint = os.getenv(\"\", \"\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")  \n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication    \n",
    "client = AzureOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version = \"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "    \n",
    "# Generate the completion  \n",
    "def chat_completion_4o(USER_MSG):\n",
    "    completion = client.chat.completions.create(  \n",
    "        model=deployment,\n",
    "        messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are going to identify the corresponding sense tag of an ambiguous word.\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": USER_MSG\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "] ,\n",
    "        max_tokens=500,  \n",
    "        temperature=0.0,\n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,\n",
    "        stop=None,  \n",
    "        stream=False\n",
    "        )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1143a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together() # auth defaults to os.environ.get(\"TOGETHER_API_KEY\")\n",
    "\n",
    "def chat_completion_deepseek(user_msg: str) -> str:   \n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"deepseek-ai/DeepSeek-V3\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": user_msg\n",
    "        }\n",
    "      ]\n",
    "  )\n",
    "  return(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf95735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the meaning of the word using GLossGPT prompt template\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_model_output(Question):\n",
    "  response = client.chat.completions.create(\n",
    "    model='virtuoso-large',\n",
    "    messages=[{'role': 'user', 'content': Question}],\n",
    "    temperature=0.4,\n",
    "  )\n",
    "\n",
    "  return(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key\n",
    "genai.configure(api_key=\"\") # Or it will pick up from GEMINI_API_KEY env var\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "def get_gemini_response(question):\n",
    "\n",
    "    response = model.generate_content(question)   \n",
    "    return(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASELINE For the Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''Your task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "Do all the reasoning internally. Provide only the final integer score '''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_gemini_response(prompt)\n",
    "            clean_text = full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {clean_text}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5116fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''Your task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "Do all the reasoning internally. Provide only the final integer score '''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            clean_text = full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {clean_text}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bcbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''Your task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "Do all the reasoning internally. Provide only the final integer score '''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_deepseek(prompt)\n",
    "            clean_text = full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {clean_text}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef06ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_model_output(prompt)\n",
    "            print(f\"Line {line_num}: {full_text.replace('\\n', ' ')}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c03215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_predictions_to_jsonl(numbers, file_path):\n",
    "    \"\"\"\n",
    "    Saves a list of numbers to a JSONL file in the format:\n",
    "    {\"id\": \"<index>\", \"prediction\": <number>}\n",
    "    \n",
    "    Args:\n",
    "        numbers (list): List of numbers (predictions).\n",
    "        file_path (str): Path to output .jsonl file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for idx, num in enumerate(numbers):\n",
    "            record = {\n",
    "                \"id\": str(idx),\n",
    "                \"prediction\": num\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"test_results/gpt40.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        elem = i.strip().split(\" \")\n",
    "        if elem[0] == \"Line\":\n",
    "            predicts.append(int(elem[2]))\n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "# save_predictions_to_jsonl(predicts, \"predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd576295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gold_final_score = []\n",
    "with open('dev.csv', 'r', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    for index, row in df.iterrows():\n",
    "        average = row['average']\n",
    "        gold_final_score.append(average)       \n",
    "        \n",
    "\n",
    "print(gold_final_score)\n",
    "print(len(gold_final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af888dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5747703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_train_with_scores.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "            average = round(float(average))\n",
    "            human_label =row[\"human_label\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            if human_label == \"HumanUnlikely\":\n",
    "                prompt=f'''\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "**Plausibility Score:** {average}'''\n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "                prompt_updated = prompt.replace('\\n', ' ')\n",
    "                print(f\"{prompt_updated}\")\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3be60",
   "metadata": {},
   "source": [
    "# Gemini Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45054c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_model_output_gemini(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "            time.sleep(20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_number_at_end(sentence: str):\n",
    "    \"\"\"\n",
    "    Returns the number found at the end of a given sentence.\n",
    "    Handles cases like **5**, (5), or 5.\n",
    "    Returns None if no number is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'[\\*\\(\\[\\{]*(-?\\d+(?:\\.\\d+)?)[\\*\\)\\]\\}]*\\s*$', sentence)\n",
    "    if match:\n",
    "        num_str = match.group(1)\n",
    "        # Return int if itâ€™s whole, float otherwise\n",
    "        return int(num_str) if num_str.isdigit() or (num_str.startswith('-') and num_str[1:].isdigit()) else float(num_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"zeroshot_Gemini.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        no = get_number_at_end(i.strip())\n",
    "        predicts.append(no)\n",
    "        #(no)\n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "#save_predictions_to_jsonl(predicts, \"predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3 vector strores for each case\n",
    "# extract one example based on the input\n",
    "# create a few shot example to simulate the few shot output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e37696",
   "metadata": {},
   "source": [
    "# Single annotator agreement with Virtuso-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and labels\n",
    "file_paths = {\n",
    "    \"ambiguous\": \"human_ambigous.txt\",\n",
    "    \"easy\": \"human_easy.txt\",\n",
    "    \"unlikely\": \"human_unlikely.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619963c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(file_path):\n",
    "    \"\"\"Load and split a text file using \\n as separator.\"\"\"\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\"],\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf auth whoami\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs, store_name):\n",
    "    \"\"\"Create and save a FAISS vector store using BGE-small embeddings.\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "        model_kwargs={'device': 'cpu'},  # change to 'cuda' if you have GPU\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "    vector_store.save_local(f\"vectorstore_{store_name}\")\n",
    "    return vector_store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37941cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stores = {}\n",
    "for label, path in file_paths.items():\n",
    "    docs = load_and_split(path)\n",
    "    vector_stores[label] = create_vector_store(docs, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_all_stores(query, k=1):\n",
    "    \"\"\"Retrieve top-k results from each vector store.\"\"\"\n",
    "    results = {}\n",
    "    for label, store in vector_stores.items():\n",
    "        docs = store.similarity_search(query, k=k)\n",
    "        results[label] = docs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(store_name):\n",
    "    \"\"\"Load an existing FAISS vector store from disk.\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "        model_kwargs={'device': 'cpu'},  # change to 'cuda' if you have GPU\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    return FAISS.load_local(\n",
    "        f\"vectorstore_{store_name}\",\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # required for FAISS in some LangChain versions\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load all your stores\n",
    "# -------------------------------\n",
    "vector_stores = {\n",
    "    \"ambiguous\": load_vector_store(\"ambiguous\"),\n",
    "    \"easy\": load_vector_store(\"easy\"),\n",
    "    \"unlikely\": load_vector_store(\"unlikely\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query):\n",
    "    \"\"\"\n",
    "    Retrieve one document (k=1) from each vector store\n",
    "    and return them as strings.\n",
    "    \"\"\"\n",
    "    retrieved = retrieve_from_all_stores(query, k=1)\n",
    "\n",
    "    # Create a dict of store_name -> retrieved text\n",
    "    retrieved_texts = {}\n",
    "    for label, docs in retrieved.items():\n",
    "        if docs:\n",
    "            retrieved_texts[label] = docs[0].page_content.strip()\n",
    "        else:\n",
    "            retrieved_texts[label] = \"\"\n",
    "\n",
    "    # Optionally, combine all into a single string\n",
    "    combined_text = \"\\n\\n\".join(\n",
    "        [f\"### {label.upper()} CONTEXT ###\\n{content}\"\n",
    "         for label, content in retrieved_texts.items()]\n",
    "    )\n",
    "\n",
    "    return combined_text, retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, k=2):\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents from each vector store\n",
    "    and return them as combined and structured strings.\n",
    "    \"\"\"\n",
    "    retrieved = retrieve_from_all_stores(query, k=k)\n",
    "\n",
    "    retrieved_texts = {}\n",
    "    for label, docs in retrieved.items():\n",
    "        if docs:\n",
    "            # Join all retrieved chunks for that store\n",
    "            combined_chunks = \"\\n---\\n\".join([d.page_content.strip() for d in docs])\n",
    "            retrieved_texts[label] = combined_chunks\n",
    "        else:\n",
    "            retrieved_texts[label] = \"\"\n",
    "\n",
    "    # Combine everything into one formatted string\n",
    "    combined_text = \"\\n\\n\".join(\n",
    "        [f\"### {label.upper()} CONTEXT ###\\n{content}\"\n",
    "         for label, content in retrieved_texts.items() if content]\n",
    "    )\n",
    "\n",
    "    return combined_text, retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents from each vector store\n",
    "    and return them as combined and structured strings.\n",
    "    \"\"\"\n",
    "    retrieved = retrieve_from_all_stores(query, k=k)\n",
    "\n",
    "    retrieved_texts = {}\n",
    "    for label, docs in retrieved.items():\n",
    "        if docs:\n",
    "            # Join all retrieved chunks for that store\n",
    "            combined_chunks = \"\\n---\\n\".join([d.page_content.strip() for d in docs])\n",
    "            retrieved_texts[label] = combined_chunks\n",
    "        else:\n",
    "            retrieved_texts[label] = \"\"\n",
    "\n",
    "    # Combine everything into one formatted string\n",
    "    combined_text = \"\\n\\n\".join(\n",
    "        [f\"### {label.upper()} CONTEXT ###\\n{content}\"\n",
    "         for label, content in retrieved_texts.items() if content]\n",
    "    )\n",
    "\n",
    "    return combined_text, retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3829d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a pair of parallel rails providing a runway for wheels,The detectives arrived at the abandoned train station. They were looking for signs of the missing artifact. A faint trail caught their attention.,They followed the <WSD>track</WSD>.,They began to run along the abandoned railway line, hopping from wooden sleeper to sleeper to avoid twisting an ankle.How do humans react to ambiguous situations?\"\n",
    "combined_text, retrieved_texts = rag_pipeline(query, k=1)\n",
    "\n",
    "print(\"ðŸ”¹ Combined Text:\\n\")\n",
    "print(combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "5.  Return only the final integer score.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected. Expected output is only a single integer score from 1 to 5.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_gemini_response(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected..'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_deepseek(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected..'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected..'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea46830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected..'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_model_output_gemini(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df88cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_test.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected..'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_deepseek(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Perform a proper reasoning and only provide the final integer. Any explanation and discussions are not expected..'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_deepseek(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Few_shot.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        elem = i.strip().split(\" \")\n",
    "        if elem[0] == \"Line\":\n",
    "            predicts.append(int(elem[2]))\n",
    "print(predicts)\n",
    "print(len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gold_final_score = []\n",
    "with open('dev.csv', 'r', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    for index, row in df.iterrows():\n",
    "        average = row['average']\n",
    "        gold_final_score.append(average)       \n",
    "        \n",
    "\n",
    "print(gold_final_score)\n",
    "print(len(gold_final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94964c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_model_output(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a59a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Few_shot_k2Virtuoso.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        elem = i.strip().split(\" \")\n",
    "        if elem[0] == \"Line\":\n",
    "            predicts.append(int(elem[2]))\n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "save_predictions_to_jsonl(predicts, \"predictions_arcee_k2.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1acbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba6e68",
   "metadata": {},
   "source": [
    "# GPT 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion(prompt)\n",
    "            full_text_fin=full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {full_text_fin}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e995ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gold_final_score = []\n",
    "with open('dev.csv', 'r', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    for index, row in df.iterrows():\n",
    "        average = row['average']\n",
    "        gold_final_score.append(average)       \n",
    "        \n",
    "\n",
    "print(gold_final_score)\n",
    "print(len(gold_final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d98279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_last_number_from_last_sentence(paragraph):\n",
    "    # Split paragraph into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', paragraph.strip())\n",
    "    \n",
    "    if not sentences:\n",
    "        return None\n",
    "    \n",
    "    # Get the last sentence\n",
    "    last_sentence = sentences[-1]\n",
    "    \n",
    "    # Find all numbers (integers or floats)\n",
    "    numbers = re.findall(r'\\d+(?:\\.\\d+)?', last_sentence)\n",
    "    \n",
    "    if numbers:\n",
    "        # Take the last number found\n",
    "        num_str = numbers[-1]\n",
    "        return float(num_str) if '.' in num_str else int(num_str)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f587f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Zeroshot_GPT4o.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        \n",
    "        #print(extract_number_from_last_sentence(i))\n",
    "        predicts.append(extract_last_number_from_last_sentence(i))\n",
    "        \n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "#save_predictions_to_jsonl(predicts, \"predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding incorrect indices\n",
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "\n",
    "print(f\"Mean Absolute Difference: {difference / len(gold_final_score):.4f}\")\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "# Calculate standard deviation of gold scores\n",
    "std_dev = np.std(gold_array)\n",
    "tolerance = std_dev  # within 1 standard deviation\n",
    "\n",
    "# Determine which predictions are within 1 std deviation\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")\n",
    "\n",
    "# Print all indices where prediction is NOT within tolerance\n",
    "incorrect_indices = np.where(~within_std)[0]\n",
    "\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(\"\\nIndices not within 1 Std Dev:\")\n",
    "    for idx in incorrect_indices:\n",
    "        print(f\"Index {idx}: gold = {gold_array[idx]:.4f}, predict = {pred_array[idx]:.4f}, diff = {abs(gold_array[idx] - pred_array[idx]):.4f}\")\n",
    "else:\n",
    "    print(\"\\nAll predictions are within 1 Std Dev.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef216779",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38b475",
   "metadata": {},
   "source": [
    "- Rag based Evaluation for GPT 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c195301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c591e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "            #print(combined_text)\n",
    "\n",
    "            prompt=f'''our task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b89b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Fewshot_k1_gpt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        \n",
    "        #print(extract_last_number_from_last_sentence(i))\n",
    "        predicts.append(extract_last_number_from_last_sentence(i))\n",
    "        \n",
    "print(predicts)\n",
    "print(len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Fewshot_k2_gpt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        \n",
    "        #print(extract_last_number_from_last_sentence(i))\n",
    "        predicts.append(extract_last_number_from_last_sentence(i))\n",
    "        \n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "save_predictions_to_jsonl(predicts, \"predictions_gpt_k2.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gemini\n",
    "import time\n",
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''Your task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = get_model_output_gemini(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"FewShot_k2_Gemini.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        no = get_number_at_end(i.strip())\n",
    "        predicts.append(no)\n",
    "        #print(no)\n",
    "print(predicts)\n",
    "print(len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e54195",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predicts=[]\n",
    "gold_final_score = []\n",
    "with open('dev.csv', 'r', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    for index, row in df.iterrows():\n",
    "        average = row['average']\n",
    "        point = round(average)\n",
    "        predicts.append(point)\n",
    "        gold_final_score.append(average)       \n",
    "        \n",
    "\n",
    "print(gold_final_score)\n",
    "print(len(gold_final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d073d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O4 Mini : k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "\n",
    "            pre_context = f''' **Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Homonym Word:** \"{homonym}'''\n",
    "            \n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''Your task is to rate the plausibility of a word's meaning on a scale of 1-5 based on a short story. Remember you will simulate the human reasoning and rating process by carefully considering the context provided.\n",
    "\n",
    "            You will be given:\n",
    "- A short story (including *Precontext*, *Sentence*, and *Ending*)\n",
    "- A *target word* (homonym)\n",
    "- A *proposed meaning* for that word\n",
    "\n",
    "Below are examples that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "**Story:**\n",
    "- Precontext: {precontext}\n",
    "- Sentence: {sentence}\n",
    "- Ending: {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Analyze the Context:** Read the complete story and identify all clues that might support or contradict the 'Proposed Meaning'.\n",
    "2.  **List Evidence For:** State the parts of the story that make the 'Proposed Meaning' plausible.\n",
    "3.  **List Evidence Against:** State any parts of the story that make the 'Proposed Meaning' implausible.\n",
    "4.  **Synthesize and Score:** Based on the evidence, provide a final plausibility score using the rubric below.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **5:** Perfectly plausible. The meaning is strongly and directly supported by multiple clues in the context.\n",
    "- **4:** Very plausible. The meaning fits well and is consistent with the context.\n",
    "- **3:** Moderately plausible. The meaning is possible but not strongly implied; the context is ambiguous.\n",
    "- **2:** Barely plausible. The meaning largely conflicts with the context but isn't entirely impossible.\n",
    "- **1:** Implausible. The meaning is directly contradicted by the context.\n",
    "\n",
    "\n",
    "Provide only the final integer score after your reasoning.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_o4(prompt)\n",
    "            output_updated = full_text.replace('\\n', ' ')             \n",
    "            print(f\"Line {line_num}: {output_updated}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_number_at_end(sentence: str):\n",
    "    \"\"\"\n",
    "    Returns the number found at the end of a given sentence.\n",
    "    Handles cases like **5**, (5), or 5.\n",
    "    Returns None if no number is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'[\\*\\(\\[\\{]*(-?\\d+(?:\\.\\d+)?)[\\*\\)\\]\\}]*\\s*$', sentence)\n",
    "    if match:\n",
    "        num_str = match.group(1)\n",
    "        # Return int if itâ€™s whole, float otherwise\n",
    "        return int(num_str) if num_str.isdigit() or (num_str.startswith('-') and num_str[1:].isdigit()) else float(num_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c46563",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Few_shot_k5_gpt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        no = get_number_at_end(i.strip())\n",
    "        predicts.append(no)\n",
    "        #print(no)\n",
    "print(predicts)\n",
    "print(len(predicts))\n",
    "save_predictions_to_jsonl(predicts, \"predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "\n",
    "            prompt=f'''You are asked to rate how plausible a proposed meaning of a target word (a homonym) is in a story. Use careful, structured reasoning but only output a single integer (1, 2, 3, 4, or 5) â€” nothing else.\n",
    "                    You will be given: \n",
    "                     - A short story (including *Precontext*, *Sentence*, and *Ending* :may be NO ENDING if not present)\n",
    "                     - A *target word* (homonym)\n",
    "                     - A *proposed meaning* for that word                    \n",
    "\n",
    "Task: Decide how plausible the Judged Meaning of the Homonym is in the story context, and output only one integer (1â€“5) as the final plausibility score.\n",
    "Required procedure are provided after the story.(do all steps mentally / privately â€” do not print them; print only the final integer):\n",
    "\n",
    "Input fields:\n",
    "                    Precontext: {precontext}\n",
    "                    Sentence (contains homonym): {sentence}\n",
    "                    Ending: {ending} \n",
    "                    Homonym: {homonym}\n",
    "                    Judged Meaning: {judged_meaning}\n",
    "\n",
    "\n",
    "1. Verify context completeness & note missing ending: If ending is NO ENDING (missing), treat contextual evidence as weaker; avoid overconfident extremes unless the sentence+precontext strongly selects one sense. Missing ending â†’ introduce slight uncertainty bias (prefer 3 over 4/2 unless strong cues exist).\n",
    "2. Identify literal vs figurative / idiomatic cues: \n",
    "- Look for physical objects, locations, bodily references, motion verbs, measurement words â†’ favors literal senses.\n",
    "- Look for evaluative/adjectival phrases, emotions, idiomatic constructions, metaphors, or social roles â†’ favors figurative/abstract senses.\n",
    "3. Search for collocational and syntactic signals:\n",
    "- Nearby nouns, verbs, prepositions or objects that commonly select one sense (e.g., â€œrail,â€ â€œsleeper,â€ â€œrailwayâ€ â†’ rail sense of track; â€œdribblingâ€ + â€œsoccer ballâ€ â†’ sports sense).\n",
    "- If the sentence itself strongly selects a sense (subject/object congruence), weight it heavily.\n",
    "4. Weigh ending vs precontext vs sentence:\n",
    "-Rank evidence strength: Sentence (strongest) > Precontext > Ending (if present).\n",
    "-However, a specific ending that explicitly implies a sense should override ambiguous sentence cues.\n",
    "5. Corner-case rules (explicit tie-breakers and calibration):\n",
    "- If both senses are clearly plausible (balanced evidence) â†’ score 3 (moderate plausibility).\n",
    "- If evidence slightly favors the judged meaning but not strongly â†’ score 4.\n",
    "- If evidence slightly disfavors it but does not contradict â†’ score 2.\n",
    "- If judged meaning is directly contradicted by sentence or precontext â†’ score 1.\n",
    "- If there is strong, multiple-clue support (sentence + precontext and/or explicit ending) â†’ score 5.\n",
    "- If ending == NO ENDING and cues are mixed â†’ bias towards 3 (avoid 4/2 unless strong single-source evidence).\n",
    "6. If story parts conflict (e.g., precontext implies one sense, ending another): Use 3 unless one part is very explicit â€” then choose 2 or 4.\n",
    "7. If unsure or context is weak: Default to 3 rather than guessing.\n",
    "\n",
    "Scoring rubric reminder (for internal use only):\n",
    "5 = Perfectly plausible; strong direct support.\n",
    "4 = Very plausible; fits well, consistent.\n",
    "3 = Moderately plausible / ambiguous.\n",
    "2 = Barely plausible; largely conflicts but not impossible.\n",
    "1 = Implausible; directly contradicted. \n",
    "\n",
    "Final output rule:\n",
    "After reasoning, output exactly one integer: 1, 2, 3, 4, or 5. No text, no punctuation, no explanation.'''                       \n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            full_text_fin=full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {full_text_fin}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffed061",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"Zeroshot_gpt_updated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        no = i.split(\":\")[1].strip()\n",
    "        no= int(no)\n",
    "        predicts.append(no)\n",
    "        #print(no)\n",
    "print(predicts)\n",
    "print(len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f550ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb890b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def main():\n",
    "    file_path = 'Updated_dev.csv'\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        line_num = 1\n",
    "        for row in reader:\n",
    "            # Assign values from each row to variables\n",
    "            homonym = row[\"homonym\"]\n",
    "            judged_meaning = row[\"judged_meaning\"]\n",
    "            precontext = row[\"precontext\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            ending = row[\"ending\"]\n",
    "            choice = row[\"choices\"]\n",
    "            example_sentence = row[\"example_sentence\"]\n",
    "            average = row[\"average\"]\n",
    "\n",
    "            if ending.strip() == \"\":\n",
    "                ending = \"[NO ENDING]\"\n",
    "            \n",
    "            pre_context = f''' **Story:**  Precontext: {precontext} - Sentence: {sentence} - Ending: {ending}  Homonym Word: \"{homonym}'''\n",
    "            combined_text, retrieved_texts = rag_pipeline(pre_context)\n",
    "\n",
    "            prompt=f'''Your task is to rate the plausibility of a word's meaning on a scale of 1-5 \n",
    "based on a short story. You must follow the Thinking Process below to arrive at your score.\n",
    "\n",
    "You will be provided with few example stories that illustrate the scoring rubric for different levels of plausibility, based on similar stories: {combined_text}.\n",
    "\n",
    "Now evaluate the following story and proposed meaning:\n",
    "\n",
    "**Story:**\n",
    "* **Precontext:** {precontext}\n",
    "* **Sentence:** {sentence}\n",
    "* **Ending:** {ending}\n",
    "\n",
    "**Word:** \"{homonym}\"\n",
    "**Proposed Meaning to Evaluate:** \"{judged_meaning}\"\n",
    "\n",
    "### Thinking Process & Response Format\n",
    "\n",
    "*Complete each step of this process in your analysis.*\n",
    "\n",
    "**Step 1: Initial Context Analysis**\n",
    "* Read the **Precontext** *alone*. Which meaning(s) of \"{homonym}\" does it make you expect?\n",
    "* Read the **Ambiguous Sentence** *alone*. Is the homonym part of a common idiom (e.g., \"had guts\") or a set phrase (e.g., \"drawing water\")?\n",
    "\n",
    "**Step 2: The Ending as a Clarifier**\n",
    "* Now, read the **Ending**. How does this new information *shift* the story?\n",
    "* Does the Ending strongly *confirm* one meaning, *contradict* another, or introduce a *twist*?\n",
    "* **Crucially:** Does the Ending act as a *paraphrase* or *definition* of a figurative or idiomatic meaning (e.g., an ending of \"She was very brave\" is a direct definition for the meaning \"fortitude\")?\n",
    "\n",
    "**Step 3: Holistic Coherence Check**\n",
    "* Read all three parts (Precontext, Sentence, Ending) as a *single, coherent narrative*.\n",
    "* Can all parts co-exist with the **Proposed Meaning**? Or do some parts create a conflict?\n",
    "* **Supporting Clues:** List the specific words/phrases from the *entire* story (Precontext, Sentence, or Ending) that support the **Proposed Meaning**.\n",
    "* **Conflicting Clues:** List the specific words/phrases that contradict or make the **Proposed Meaning** awkward or implausible.\n",
    "\n",
    "**Step 4: Final Score and Justification**\n",
    "* Based on your coherence check in Step 3, assign a score.\n",
    "* *Pay close attention to contradictions*: If a strong clue in the Ending (e.g., \"wetness on his chin\") directly conflicts with a meaning (\"propel a ball\"), the score must be low, *even if* the precontext (\"soccer ball\") supported it.\n",
    "* *Pay close attention to modifiers*: If the sentence just describes *how* an action is done (e.g., \"pick *without discernment*\"), the core action (\"select\") is still plausible.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "* **5: Perfectly plausible.** The meaning is strongly supported by the *entire* context, and all parts of the story (Precontext, Sentence, Ending) form a consistent, logical narrative.\n",
    "* **4: Very plausible.** The meaning fits well and is consistent. There might be minor ambiguity, but no real contradictions.\n",
    "* **3: Moderately plausible.** The meaning is possible, but the context is ambiguous or contains *minor* conflicting clues that make the story a bit strange.\n",
    "* **2: Barely plausible.** The meaning *largely* conflicts with the context. You have to ignore a key piece of information (especially in the Ending) to make it fit.\n",
    "* **1: Implausible.** The meaning is directly and strongly contradicted by the context (e.g., the Ending makes it physically impossible or nonsensical).\n",
    "\n",
    "Do all the reasoning *mentally* / *privately* â€” do not print it; print only the final integer score as a output.\n",
    "'''\n",
    "        \n",
    "                    \n",
    "            # Generate response using the same format as training\n",
    "            full_text = chat_completion_4o(prompt)\n",
    "            full_text_fin=full_text.replace('\\n', ' ')\n",
    "            print(f\"Line {line_num}: {full_text_fin}\")\n",
    "            line_num += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a594a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "with open(\"fewshot_updated_gpt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        no = i.split(\":\")[1].strip()\n",
    "        no= int(no)\n",
    "        predicts.append(no)\n",
    "        #print(no)\n",
    "print(predicts)\n",
    "print(len(predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0395aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the distribution of predicted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gold_predicts=[]\n",
    "gold_final_score = []\n",
    "with open('dev.csv', 'r', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    for index, row in df.iterrows():\n",
    "        average = row['average']\n",
    "        point = round(average)\n",
    "        gold_predicts.append(point)\n",
    "        gold_final_score.append(average)       \n",
    "        \n",
    "\n",
    "print(gold_final_score)\n",
    "print(len(gold_final_score))\n",
    "print(gold_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1566e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "one=0\n",
    "two=0\n",
    "three=0\n",
    "four=0\n",
    "five=0\n",
    "for i in gold_predicts:\n",
    "    if i==1:\n",
    "        one+=1\n",
    "    elif i==2:\n",
    "        two+=1\n",
    "    elif i==3:\n",
    "        three+=1\n",
    "    elif i==4:\n",
    "        four+=1\n",
    "    elif i==5:\n",
    "        five+=1\n",
    "print(f\"1: {one}, 2: {two}, 3: {three}, 4: {four}, 5: {five}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "one=0\n",
    "two=0\n",
    "three=0\n",
    "four=0\n",
    "five=0\n",
    "for i,j in zip(gold_predicts,gptpredicts):\n",
    "    if i==1 and j==1:\n",
    "        one+=1\n",
    "    elif i==2 and j==2:\n",
    "        two+=1\n",
    "    elif i==3 and j==3:\n",
    "        three+=1\n",
    "    elif i==4 and j==4:\n",
    "        four+=1\n",
    "    elif i==5 and j==5:\n",
    "        five+=1\n",
    "print(f\"1: {one}, 2: {two}, 3: {three}, 4: {four}, 5: {five}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "virpredicts=[]\n",
    "with open(\"Zeroshot_Virtuoso.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        elem = i.strip().split(\" \")\n",
    "        if elem[0] == \"Line\":\n",
    "            virpredicts.append(int(elem[2]))\n",
    "print(virpredicts)\n",
    "print(len(virpredicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gempredicts=[]\n",
    "with open(\"zeroshot_Gemini.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        no = get_number_at_end(i.strip())\n",
    "        gempredicts.append(no)\n",
    "        #(no)\n",
    "print(gempredicts)\n",
    "print(len(gempredicts))\n",
    "#save_predictions_to_jsonl(gempredicts, \"predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptpredicts=[]\n",
    "with open(\"Zeroshot_GPT4o.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f:\n",
    "        \n",
    "        #print(extract_number_from_last_sentence(i))\n",
    "        gptpredicts.append(extract_last_number_from_last_sentence(i))\n",
    "\n",
    "print(gptpredicts)\n",
    "print(len(gptpredicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions_distribution(gold_predicts, virpredicts, gempredicts, gptpredicts):\n",
    "    \"\"\"\n",
    "    Plots a line graph comparing the distributions of four prediction result lists:\n",
    "    - gold_predicts: Ground truth values\n",
    "    - virpredicts: Virtuosa Large model predictions\n",
    "    - gempredicts: Gemini model predictions\n",
    "    - gptpredicts: GPT-40 model predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot each line\n",
    "    plt.plot(gold_predicts, label='Golden Results', marker='o')\n",
    "    plt.plot(virpredicts, label='Virtuosa Large', marker='s')\n",
    "    plt.plot(gempredicts, label='Gemini', marker='^')\n",
    "    plt.plot(gptpredicts, label='GPT-40', marker='d')\n",
    "    \n",
    "    # Add labels, title, legend, and grid\n",
    "    plt.title('Model Predictions Distribution Comparison')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Prediction Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Improve layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_distribution(gold_predicts, virpredicts, gempredicts, gptpredicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions_hist(gold_predicts, virpredicts, gempredicts, gptpredicts):\n",
    "    \"\"\"\n",
    "    Plot histogram distributions for four prediction lists.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.hist(gold_predicts, bins=range(1, 7), alpha=0.5, label='Golden Results', edgecolor='black')\n",
    "    plt.hist(virpredicts, bins=range(1, 7), alpha=0.5, label='Virtuosa Large', edgecolor='black')\n",
    "    plt.hist(gempredicts, bins=range(1, 7), alpha=0.5, label='Gemini', edgecolor='black')\n",
    "    plt.hist(gptpredicts, bins=range(1, 7), alpha=0.5, label='GPT-40', edgecolor='black')\n",
    "\n",
    "    plt.title('Distribution of Prediction Scores')\n",
    "    plt.xlabel('Prediction Value (1â€“5)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_hist(gold_predicts, virpredicts, gempredicts, gptpredicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "for i,j,k in zip(virpredicts, gempredicts, gptpredicts):\n",
    "    avg = (i + j + k) / 3\n",
    "    final_point = round(avg)\n",
    "    predicts.append(final_point)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "for i,j,k in zip(virpredicts, gempredicts, gptpredicts):\n",
    "    list_vals = [i, j, k]\n",
    "    min_val = min(list_vals)   \n",
    "    predicts.append(min_val)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "for i,j,k in zip(virpredicts, gempredicts, gptpredicts):\n",
    "    list_vals = [i, j, k]\n",
    "    max_val = max(list_vals)\n",
    "    predicts.append(max_val)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adfb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = 0\n",
    "for gold, predict in zip(gold_final_score, predicts):\n",
    "    diff = abs(gold - predict)\n",
    "    difference += diff\n",
    "    \n",
    "print(difference/len(gold_final_score))\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# gold_final_score and Gemma_final_score are lists of equal length\n",
    "spearman_corr, _ = spearmanr(gold_final_score, predicts)\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gold_array = np.array(gold_final_score)\n",
    "pred_array = np.array(predicts)\n",
    "\n",
    "std_dev = np.std(gold_array)  # global standard deviation across gold scores\n",
    "tolerance = std_dev  # \"within 1 std deviation\"\n",
    "\n",
    "within_std = np.abs(pred_array - gold_array) <= tolerance\n",
    "accuracy_within_std = np.mean(within_std)\n",
    "\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")\n",
    "print(f\"Accuracy within 1 Std Dev: {accuracy_within_std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
